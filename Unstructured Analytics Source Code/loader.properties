# *****************
# Loader properties
# *****************
# loader.include.* are whether to include specific portions in the load.
# These properties will control the OTCA request, how the OTCA result is processed, and what ultimately gets loaded
# Defaults: documentSentiment = true, sentenceSentiment = false, summary = false, concepts = true, subConcepts = false,
# categories = true, terms = true, geoTerms = true, subTerms = false
loader.include.documentSentiment=true
loader.include.sentenceSentiment=true
loader.include.summary=true
loader.include.concepts=true
loader.include.subConcepts=true
loader.include.categories=true
loader.include.terms=true
loader.include.geoTerms=true
loader.include.subTerms=true

# loader.maintenanceInterval specifies the interval (# of documents) at which to run the sql.maintenance.* commands. 0 means to never run. Default is 0
loader.maintenanceInterval=10000
# loader.updateDocuments if true will check if the document already exists, and update changed content only rather than delete / insert.  Default is true
loader.updateDocuments=true
# loader.name.docId is the name of the document id field in the database to load. Default is doc_id
loader.name.docId=doc_id
# loader.name.content is the name of the document content field in the database to load. Default is content
loader.name.content=content


# *****************
# Source properties
# *****************
# source.dsId is the value used when loading new documents as the document source (dsId). If a source id is found in the source, this value is not used. Default is 1
source.dsId=1
# source.type specifies the type of source to use for available documents.  Can be solr, jdbc, file, or zip.  Default is solr
source.type=jdbc


# **********************
# Source Solr properties
# **********************
# when source.type is solr these properties will be used to fetch documents from Solr
# source.solr.url is the Solr API endpoint
source.solr.url=http://localhost:8983/solr/documents
# source.solr.query is the actual query command to issue to Solr to get the list of documents
source.solr.query=*:*
# source.solr.name.* specifies the names of the fields within solr. dsId and date are optional
source.solr.name.dsId=source
source.solr.name.origId=dsId
source.solr.name.content=text
source.solr.name.date=doc_date
# source.solr.sort is the sort to apply to the results, this is important if records are added after the search is started
source.solr.sort=last_updated asc
# source.solr.rows is the number of rows to return per page of results. This affects performance. Default is 100
source.solr.rows=100


# **********************
# Source JDBC properties
# **********************
# when source.type is jdbc these properties will be used to load the JDBC driver and connect to the database
source.jdbc.driver=com.mysql.jdbc.Driver
source.jdbc.url=jdbc:mysql://localhost:3306/unstructured_documents?allowMultiQueries=true
source.jdbc.username=root
source.jdbc.password=
# source.jdbc.sql is used as the query to get documents
source.jdbc.sql=SELECT ds_id, orig_id, content, doc_date FROM documents WHERE last_processed IS NULL
# source.jdbc.name.* specifies the names of the fields in the jdbc query. dsId and date are optional
source.jdbc.name.dsId=ds_id
source.jdbc.name.origId=orig_id
source.jdbc.name.content=content
source.jdbc.name.date=doc_date


# **********************
# Source File properties
# **********************
# when source.type is file these properties will be used to read documents from the file system
# source.file.path is the file system path where the files are located to be processed as documents
source.file.path=/var/source_documents
# source.file.filter is the optional file mask wildcard filter when getting the list of files to process as documents. Default is *
source.file.filter=*



# *********************
# Source Zip properties
# *********************
# when source.type is zip these properties will be used to read documents from the zip file
# source.zip.path is the full path for the zip file to be loaded
source.zip.path=/test.zip
# source.zip.filter is the optional file mask wildcard filter when getting the list of files to process as documents from within the zip file. Default is *
source.zip.filter=*



# ***************
# OTCA properties
# ***************
# otca.url is the REST API endpoint for OTCA
otca.url=http://ot-election-otca.eastus.cloudapp.azure.com:40002/rs/
# otca.summary.percentage is the percentage of the document to use for the summary. Requires loader.include.summary be true. Default is 15
otca.summary.percentage=15
# otca.summary.sentences is the number of sentences to use for the summary. Requires loader.include.summary be true. If 0 or not specified otca.summary.percentage is used
otca.summary.sentences=0
# otca.summary.kbid is the Knowledge Base ID to use for the summary. Requires loader.include.summary be true. Default is IPTC
otca.summary.kbid=IPTC
# otca.concepts.simple is the number of simple concepts to include. Requires loader.include.concepts be true. Default is 5
otca.concepts.simple=10
# otca.concepts.complex is the number of complex concepts to include. Requires loader.include.concepts be true. Default is 5
otca.concepts.complex=10
# otca.categories.kbids is a comma separated list of knowledge base Ids to use for the categorizer. Requires loader.include.categories be true. Default is IPTC,LCTGM
otca.categories.kbids=IPTC,LCTGM
# otca.categories.number is the number of categories to include. Requires loader.include.categories by true. Default is 5
otca.categories.number=5
# otca.categories.rejected is the number of rejected categories to include. Requires loader.include.categories be true. Default is 0
otca.categories.rejected=0
# otca.finder.cartridges is a comma separated list of cartridges to be used in the Finder. Requires loader.include.finder be true. Default is ON,GL,PN,TM,EV
otca.finder.cartridges=ON,GL,PN,TM,EV
# otca.geoCartridges is a comma separated list of cartridges to use for geo processing. Requires loader.include.geoTerms be true. Default is GL
otca.geoCartridges=GL
# otca.geoTypes is a comma separated list of Type attribute values to be used for geo processing. Requires loader.include.geoTerms be true. Default is Country
otca.geoTypes=State,Country
# otca.cleanTone when true, will only return: "positive", "negative", or "neutral" for tone. All other values will return as "neutral". Default is false
otca.cleanTone=true


# **************************
# JDBC Connection properties
# **************************
# These properties will be used to load the JDBC driver and connect to the database which will get loaded
jdbc.driver=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://localhost:3306/unstructured?allowMultiQueries=true
jdbc.username=root
jdbc.password=


# *********************
# Loader SQL Statements
# *********************
# The sql.* properties should never be modified unless you have a custom database schema for loading the documents
# modifying these statements will also require code changes if the parameters (?) are changed

# sql.select.document is the sql statement used to check for the existence of a document, and get the doc_id (if it exists)
sql.select.documents=SELECT doc_id, content FROM documents WHERE ds_id = ? AND orig_id = ?
# sql.update.documentContent is the sql statement used to update the content of a given document. Requires loader.updateDocuments be true
sql.update.documents=UPDATE documents SET content = ?, length = ?, words = ?, doc_date = ? WHERE doc_id = ?
# sql.update.documentProcessed is the sql statement used to update a document as processed. Not used if left blank
sql.update.documentProcessed=UPDATE documents SET last_processed = CURRENT_TIMESTAMP WHERE doc_id = ?

# sql.insert.* are all the SQL statements that will be executed by the loader to insert new data
sql.insert.documents=INSERT INTO documents (ds_id, orig_id, content, length, words, doc_date) VALUES (?, ?, ?, ?, ?, ?)
sql.insert.sentimentDocuments=INSERT INTO ca_sentiment_documents (doc_id, tone, tone_positive_score, tone_positive_distribution, tone_negative_score, tone_negative_distribution, subjectivity, subjectivity_score, subjectivity_distribution) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
sql.insert.sentimentSentences=INSERT INTO ca_sentiment_sentences (doc_id, sentence_begin, sentence_end, sentence, tone, tone_positive_score, tone_negative_score, subjectivity, subjectivity_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
sql.insert.summaries=INSERT INTO ca_summaries (doc_id, summary) VALUES (?, ?)
sql.insert.concepts=INSERT INTO ca_concepts (doc_id, type, concept, frequency, relevancy) VALUES (?, ?, ?, ?, ?)
sql.insert.subConcepts=INSERT INTO ca_concept_subterms (doc_id, type, concept, position, length, text_part, sub_term) VALUES (?, ?, ?, ?, ?, ?, ?)
sql.insert.categories=INSERT INTO ca_categories (doc_id, kb_id, category_id, relevancy_score, weight, rejected, category) VALUES (?, ?, ?, ?, ?, ?, ?)
sql.insert.terms=INSERT INTO ca_terms (doc_id, cartridge, term_id, confidence_score, relevancy_score, frequency, subjectivity, subjectivity_confidence_score, subjectivity_score, subjectivity_distribution, tone, tone_confidence_score, tone_positive_score, tone_positive_distribution, tone_negative_score, tone_negative_distribution, main_term) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
sql.insert.geoTerms=INSERT INTO ca_terms (doc_id, cartridge, term_id, confidence_score, relevancy_score, frequency, subjectivity, subjectivity_confidence_score, subjectivity_score, subjectivity_distribution, tone, tone_confidence_score, tone_positive_score, tone_positive_distribution, tone_negative_score, tone_negative_distribution, geo_type, geo_term) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
sql.insert.subTerms=INSERT INTO ca_term_subterms (doc_id, cartridge, term_id, position, length, subjectivity, subjectivity_confidence_score, subjectivity_score, tone, tone_confidence_score, tone_positive_score, tone_negative_score, sentence_begin, sentence_end, sentence, sub_term) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)

# sql.delete.* are all the SQL statements that will be executed by the loader to delete old records before inserting new ones
sql.delete.documents=DELETE FROM documents WHERE ds_id = ? AND orig_id = ?
sql.delete.sentimentDocuments=DELETE FROM ca_sentiment_documents WHERE doc_id = ?
sql.delete.sentimentSentences=DELETE FROM ca_sentiment_sentences WHERE doc_id = ?
sql.delete.summaries=DELETE FROM ca_summaries WHERE doc_id = ?
sql.delete.concepts=DELETE FROM ca_concepts WHERE doc_id = ?
sql.delete.subConcepts=DELETE FROM ca_concept_subterms WHERE doc_id = ?
sql.delete.categories=DELETE FROM ca_categories WHERE doc_id = ?
sql.delete.terms=DELETE FROM ca_terms WHERE doc_id = ?
sql.delete.geoTerms=DELETE FROM ca_geo WHERE doc_id = ?
sql.delete.subTerms=DELETE FROM ca_term_subterms WHERE doc_id = ?

# sql.maintenance.* are the SQL statements to execute in order to perform maintenance at intervals as specified by loader.maintenanceInterval
# these commands may be needed by some RDBMS systems that require index rebuild after a lot of delete/insert statements in order to maintain performance
sql.maintenance.documents=OPTIMIZE TABLE documents
sql.maintenance.sentimentDocuments=OPTIMIZE TABLE ca_sentiment_documents
sql.maintenance.sentimentSentences=OPTIMIZE TABLE ca_sentiment_sentences
sql.maintenance.summaries=OPTIMIZE TABLE ca_summaries
sql.maintenance.concepts=OPTIMIZE TABLE ca_concepts
sql.maintenance.subConcepts=OPTIMIZE TABLE ca_concept_subterms
sql.maintenance.categories=OPTIMIZE TABLE ca_categories
sql.maintenance.terms=OPTIMIZE TABLE ca_terms
sql.maintenance.geoTerms=OPTIMIZE TABLE ca_geo
sql.maintenance.subTerms=OPTIMIZE TABLE ca_term_subterms


# ****************
# log4j properties
# ****************
# ** See log4j documentation (use Google)
log4j.rootLogger=FATAL
log4j.logger.Loader=info,stdout

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss.SSS}\t%p\t%m%n

log4j.appender.loader=com.opentext.analytics.labs.unstructured.logging.TimestampedFileAppender
log4j.appender.loader.File=logs/loader-%timestamp.log
log4j.appender.loader.layout=org.apache.log4j.PatternLayout
log4j.appender.loader.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss}\t%m%n
